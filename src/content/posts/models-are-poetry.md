---
title: "Models Are Poetry"
description: "Why the best models not only describe reality but also propose a way of seeing it."
date: 2026-02-24
draft: false
---

Model creation is fascinating because it requires the creator to do something more akin to authorship than math. They have to describe how something works, building a small, functional replica of something complex and convincing. In a way, every model is a world that requires a little suspension of disbelief like Tolkien's Middle-earth.

Humans have decent instincts about whether the story holds together which is why spurious correlations are funny. The fact that babies named Kenzie and UFO sightings in South Dakota track each other with an r-squared of 0.88 makes us laugh because the story doesn't work. We know it intuitively without ever running a test.

Other models earn their keep despite being just as fictional. Take the coin-flipping model used in some customer lifetime value calculations. The idea is that before each purchase period, a customer secretly flips a weighted coin to decide if they will leave the market forever, then flips another to decide if they'll buy during this period. Of course, nobody actually does this, yet the model captures something real about the rhythm of customer behavior that holds up empirically. Consumer behavior experts, meanwhile, would build a funnel, stage-gating customers through adoption, service, and consumption. That model also makes intuitive sense and holds up. The truth is probably some of both, and neither camp can fully prove it. What strikes me is that the best modelers, whether they think of themselves as quants or not, are doing something closer to what poets do: finding the metaphor that makes something vast feel navigable.

I've been thinking about this as I've spent more time with generative AI. My mental model started at "It's Wordle" and slowly moved to "It's fancy autocorrect". The more I learn about how these systems actually work from tokenization to encoding to calculating the probabilities of the next best token, the more I find myself genuinely struck by the creativity inside what looks like pure engineering.

Lately, I've been thinking about context windows. Despite the staggering amount of data these models have been trained on, the chat window is what actually shapes responses. That recent input carries disproportionate weight relative to everything the model has ever seen.

My first instinct was that this sounded like a design flaw. Then I thought about how Kahneman and Tversky earned a Nobel Prize out of the same observation about humans. They called it availability bias. What I didn't fully appreciate until I started working with LLMs was how much my own outputs, thinking, framing, and intuitions shift based on what I've been taking in recently. My parents and grandparents knew this long before I did. They always warned me about what I watched, what I listened to, and who I spent time with. It took me watching a language model respond differently based on its context to feel the lesson rather than just know it.

The broader implication is simpler. If recent inputs shape outputs in models and in people, then what someone is actively engaging with can be telling. It's a useful lens for understanding interesting people, for noticing changes in the people you care about, and for looking at yourself honestly when the direction you're heading doesn't match the one you intended.

Models aren't glamorous. The people who build them would generally prefer to be thought of as rigorous rather than romantic. The more time I spend with models, the more convinced I am that the best ones are a kind of poetry, which doesn't describe reality but instead proposes a new way of seeing it.
